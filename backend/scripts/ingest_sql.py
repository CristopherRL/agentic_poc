from __future__ import annotations
import json
import logging
import sqlite3
import time
from pathlib import Path
import pandas as pd
from sqlalchemy import create_engine
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from src.app.config import settings

LOGGER = logging.getLogger(__name__)


def load_csvs(csv_dir: Path) -> dict[str, pd.DataFrame]:
    frames: dict[str, pd.DataFrame] = {}
    for csv_file in sorted(csv_dir.glob("*.csv")):
        table_name = csv_file.stem.upper()
        LOGGER.info("Loading %s into table %s", csv_file.name, table_name)
        frames[table_name] = pd.read_csv(csv_file)
    if not frames:
        raise FileNotFoundError(f"No CSV files found in {csv_dir}")
    return frames


def persist_frames(frames: dict[str, pd.DataFrame], db_path: Path) -> None:
    db_path.parent.mkdir(parents=True, exist_ok=True)
    
    for table_name, frame in frames.items():
        LOGGER.info("Writing table %s (%d rows)", table_name, len(frame))
        # Create a fresh engine for each table to avoid transaction issues
        engine = create_engine(
            f"sqlite:///{db_path}", 
            connect_args={"check_same_thread": False}
        )
        try:
            # Use chunksize for large tables to avoid memory issues
            chunksize = 1000 if len(frame) > 1000 else None
            # Write table - don't use method="multi" as it can cause issues with SQLite
            frame.to_sql(
                table_name, 
                engine, 
                if_exists="replace", 
                index=False,
                chunksize=chunksize
            )
            LOGGER.info("  ✓ Successfully wrote table %s", table_name)
        finally:
            engine.dispose()
            # Give SQLite a moment to finalize writes between tables
            time.sleep(0.1)


def write_schema_report(frames: dict[str, pd.DataFrame], db_path: Path, report_path: Path) -> None:
    report_path.parent.mkdir(parents=True, exist_ok=True)
    connection = sqlite3.connect(db_path)
    try:
        cursor = connection.cursor()
        cursor.execute("SELECT name, sql FROM sqlite_master WHERE type='table' ORDER BY name")
        tables = cursor.fetchall()
    finally:
        connection.close()

    lines: list[str] = [
        "-- SQLite Schema Overview",
        "-- Generated by scripts/ingest_sql.py",
        "-- This file contains the database schema with table and column comments",
        ""
    ]
    
    for table_name, create_sql in tables:
        lines.append("-- " + "=" * 75)
        comment = settings.table_comments.get(table_name.upper(), "")
        if comment:
            lines.append(f"-- Table: {table_name}")
            lines.append(f"-- Purpose: {comment}")
        else:
            lines.append(f"-- Table: {table_name}")
        lines.append("-- " + "=" * 75)
        
        if create_sql:
            lines.append(create_sql)
            lines.append("")
            
            # Add sample data comments
            frame = frames.get(table_name.upper())
            if frame is not None and len(frame) > 0:
                sample = frame.head(2).to_dict(orient="records")
                lines.append("-- Sample data:")
                for row in sample:
                    row_str = ", ".join(f"{k}: {json.dumps(v)}" for k, v in row.items())
                    lines.append(f"--   {row_str}")
                lines.append("")
    
    # Add relationships section
    lines.append("-- " + "=" * 75)
    lines.append("-- Relationships:")
    lines.append("-- " + "=" * 75)
    lines.append("-- FACT_SALES.model_id -> DIM_MODEL.model_id")
    lines.append("-- FACT_SALES.country_code -> DIM_COUNTRY.country_code")
    lines.append("-- FACT_SALES_ORDERTYPE.model_id -> DIM_MODEL.model_id")
    lines.append("-- FACT_SALES_ORDERTYPE.country_code -> DIM_COUNTRY.country_code")
    lines.append("-- FACT_SALES_ORDERTYPE.ordertype_id -> DIM_ORDERTYPE.ordertype_id")
    lines.append("")
    
    # Add sample data examples section
    lines.append("-- " + "=" * 75)
    lines.append("-- Sample Data Examples:")
    lines.append("-- " + "=" * 75)
    
    for table_name in ["DIM_COUNTRY", "DIM_MODEL", "DIM_ORDERTYPE", "FACT_SALES", "FACT_SALES_ORDERTYPE"]:
        frame = frames.get(table_name)
        if frame is not None and len(frame) > 0:
            sample = frame.head(2).to_dict(orient="records")
            lines.append(f"-- {table_name} sample rows:")
            for row in sample:
                row_json = json.dumps(row, ensure_ascii=False)
                lines.append(f"--   {row_json}")
            lines.append("")

    report_path.write_text("\n".join(lines), encoding="utf-8")
    LOGGER.info("Schema report written to %s", report_path)


def write_sql_summary(schema_path: Path, summary_path: Path) -> None:
    """Generate a brief summary of SQL tables using LLM for user reference."""
    summary_path.parent.mkdir(parents=True, exist_ok=True)
    
    if not schema_path.exists():
        LOGGER.warning("Schema file not found at %s, skipping summary generation", schema_path)
        summary_path.write_text("SQL summary not available. Please run the SQL ingestion script.", encoding="utf-8")
        return
    
    LOGGER.info("Reading schema from %s to generate summary...", schema_path)
    schema_content = schema_path.read_text(encoding="utf-8")
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", settings.sql_summary_prompt),
        ("user", "Based on this database schema, generate a concise summary (2-3 paragraphs) explaining:\n1. What type of data is stored (e.g., sales data, vehicle information)\n2. What dimensions and facts are available\n3. What time periods or categories users can query\n\nIMPORTANT: Do not mention actual table names (like DIM_COUNTRY, FACT_SALES, etc.) or column names. Use generic descriptions instead.\n\nSchema:\n{schema}")
    ])
    
    llm = ChatOpenAI(
        model=settings.summary_llm_model,
        temperature=settings.summary_llm_temperature,
        api_key=settings.openai_api_key,
    )
    
    LOGGER.info("Generating SQL summary using LLM (model: %s, temp: %s)...", settings.summary_llm_model, settings.summary_llm_temperature)
    messages = prompt.format_prompt(schema=schema_content).to_messages()
    response = llm.invoke(messages)
    summary = (response.content or "").strip()
    
    summary_path.write_text(summary, encoding="utf-8")
    LOGGER.info("SQL summary written to %s", summary_path)


def reset_database(db_path: Path | None = None) -> None:
    target = db_path or settings.sqlite_path
    if not target.exists():
        return

    for attempt in range(3):
        try:
            target.unlink()
            LOGGER.info("Removed existing database at %s", target)
            return
        except PermissionError:
            time.sleep(0.3)

    LOGGER.warning("Could not delete %s; attempting to drop existing data tables only", target)
    connection = sqlite3.connect(str(target))
    try:
        cursor = connection.cursor()
        # Enable WAL mode to allow concurrent reads during writes
        cursor.execute("PRAGMA journal_mode=WAL")
        # Get all user tables (exclude system tables like sqlite_sequence, sqlite_master, etc.)
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' 
            AND name NOT LIKE 'sqlite_%'
        """)
        user_tables = [row[0] for row in cursor.fetchall()]
        
        if not user_tables:
            LOGGER.info("No user tables to drop")
            connection.commit()
            return
        
        # Only drop data tables, preserve system tables like rate_limit
        data_tables = set(settings.required_data_tables)
        tables_to_drop = [t for t in user_tables if t in data_tables]
        
        # Preserve rate_limit and other non-data tables
        preserved_tables = [t for t in user_tables if t not in data_tables]
        if preserved_tables:
            LOGGER.info("Preserving non-data tables: %s", ", ".join(preserved_tables))
        
        if not tables_to_drop:
            LOGGER.info("No data tables to drop (they may already be missing)")
            connection.commit()
            return
        
        for table_name in tables_to_drop:
            cursor.execute(f"DROP TABLE IF EXISTS {table_name}")
            LOGGER.info("Dropped table %s", table_name)
        connection.commit()
    finally:
        connection.close()
        # Give SQLite a moment to release locks
        time.sleep(0.1)


def ingest(csv_dir: Path | None = None, db_path: Path | None = None, schema_path: Path | None = None) -> None:
    source_dir = csv_dir or settings.csv_dir
    target_db = db_path or settings.sqlite_path
    report_path = schema_path or settings.sql_schema_path
    summary_path = target_db.parent / "sql_summary.md"
    
    # Remove old .md schema file if it exists (we now use .sql)
    old_md_path = target_db.parent / "sql_schema.md"
    if old_md_path.exists():
        old_md_path.unlink()
        LOGGER.info("  → Removed old sql_schema.md file (now using .sql)")

    LOGGER.info("Starting SQL ingestion process...")
    LOGGER.info("  Source CSV directory: %s", source_dir)
    LOGGER.info("  Target database: %s", target_db)
    
    reset_database(target_db)
    LOGGER.info("  → Database reset completed")

    LOGGER.info("  → Loading CSV files...")
    frames = load_csvs(source_dir)
    LOGGER.info("  → Persisting to database...")
    persist_frames(frames, target_db)
    LOGGER.info("  → Writing schema report...")
    write_schema_report(frames, target_db, report_path)
    LOGGER.info("  → Generating SQL summary with LLM...")
    write_sql_summary(report_path, summary_path)
    LOGGER.info("✓ SQL ingestion completed successfully. Database written to %s", target_db)


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(levelname)s %(message)s")
    ingest()
